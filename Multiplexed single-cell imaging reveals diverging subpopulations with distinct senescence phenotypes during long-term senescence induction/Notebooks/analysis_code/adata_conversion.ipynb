{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b518f9",
   "metadata": {},
   "source": [
    "This notebook is intended to be run on the output of the 4i pipeline hosted at https://github.com/PurvisLabTeam/4i_pipeline\n",
    "\n",
    "It produces a single Anndata object, with user defined metadata, for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8162ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "print(\"PYTHONPATH:\", os.environ.get('PYTHONPATH'))\n",
    "print(\"PATH:\", os.environ.get('PATH'))\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import phate\n",
    "import math\n",
    "import random\n",
    "import gc\n",
    "import scprep\n",
    "from datetime import datetime, time\n",
    "from matplotlib.animation import ImageMagickWriter\n",
    "import matplotlib.animation as animation\n",
    "import zipfile\n",
    "from urllib.request import urlopen\n",
    "import scipy.stats as st\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import kde\n",
    "from scipy.stats import binned_statistic\n",
    "from scipy.stats import f_oneway\n",
    "from matplotlib.colors import LogNorm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "print(sns.__version__)\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "from delve import *\n",
    "import anndata as ad\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kh import sketch\n",
    "from sklearn.cluster import KMeans\n",
    "import umap\n",
    "print(sc.__version__)\n",
    "today = datetime.now().strftime(\"%m%d%Y-%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d9229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### full_dir should be the directory that contains the output of your \"calculate cell properties\" notebook. Called cell_data by default\n",
    "### well_list should be every well in the dataset that you intend to combine into a single adata object for analysis\n",
    "\n",
    "full_dir = r'your/path/here.csv'\n",
    "\n",
    "well_list = ['well_designation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d6efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition to Normalize the dataframe by z-score\n",
    "\n",
    "def standardizeColumns(df):\n",
    "    df = df.copy()\n",
    "    df.iloc[:,:] = df.iloc[:,:].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383418ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame to concatenate all data\n",
    "fullest_df = pd.DataFrame()\n",
    "\n",
    "# Dictionary to store DataFrames for each well\n",
    "well_dataframes = {}\n",
    "\n",
    "for well in well_list:\n",
    "    print(f'starting Well {well}')\n",
    "    full_df = pd.read_csv(os.path.join(full_dir, f'cell_data_{well}_df.csv'), sep=',') \n",
    "\n",
    "    # Add important metadata information\n",
    "    if (\"B\" or \"C\") and \"2\" in well:\n",
    "        full_df['treatment'] = 'etop' \n",
    "        full_df['Group'] = 'control' \n",
    "        full_df['sample_id'] = 1\n",
    "    if (\"B\" or \"C\") and \"3\" in well:\n",
    "        full_df['treatment'] = 'etop' \n",
    "        full_df['Group'] = 'timepoint 1' \n",
    "        full_df['sample_id'] = 2\n",
    "    if (\"B\" or \"C\") and \"4\" in well:\n",
    "        full_df['treatment'] = 'etop' \n",
    "        full_df['Group'] = 'timepoint 2' \n",
    "        full_df['sample_id'] = 3\n",
    "    if (\"B\" or \"C\") and \"5\" in well:\n",
    "        full_df['treatment'] = 'etop' \n",
    "        full_df['Group'] = 'timepoint 3' \n",
    "        full_df['sample_id'] = 4\n",
    "    if (\"B\" or \"C\") and \"6\" in well:\n",
    "        full_df['treatment'] = 'etop' \n",
    "        full_df['Group'] = 'timepoint 4' \n",
    "        full_df['sample_id'] = 5\n",
    "    if (\"B\" or \"C\") and \"7\" in well:\n",
    "        full_df['treatment'] = 'etop' \n",
    "        full_df['Group'] = 'timepoint 5' \n",
    "        full_df['sample_id'] = 6\n",
    "    if (\"B\" or \"C\") and \"8\" in well:\n",
    "        full_df['treatment'] = 'etop' \n",
    "        full_df['Group'] = 'timepoint 6' \n",
    "        full_df['sample_id'] = 7\n",
    "    if (\"B\" or \"C\") and \"9\" in well:\n",
    "        full_df['treatment'] = 'etop' \n",
    "        full_df['Group'] = 'timepoint 7' \n",
    "        full_df['sample_id'] = 8\n",
    "    if (\"D\" or \"E\") and \"2\" in well:\n",
    "        full_df['treatment'] = 'paracrine' \n",
    "        full_df['Group'] = 'control' \n",
    "        full_df['sample_id'] = 9\n",
    "    if (\"D\" or \"E\") and \"3\" in well:\n",
    "        full_df['treatment'] = 'paracrine' \n",
    "        full_df['Group'] = 'timepoint 1' \n",
    "        full_df['sample_id'] = 10\n",
    "    if (\"D\" or \"E\") and \"4\" in well:\n",
    "        full_df['treatment'] = 'paracrine' \n",
    "        full_df['Group'] = 'timepoint 2' \n",
    "        full_df['sample_id'] = 11\n",
    "    if (\"D\" or \"E\") and \"5\" in well:\n",
    "        full_df['treatment'] = 'paracrine' \n",
    "        full_df['Group'] = 'timepoint 3' \n",
    "        full_df['sample_id'] = 12\n",
    "    if (\"D\" or \"E\") and \"6\" in well:\n",
    "        full_df['treatment'] = 'paracrine' \n",
    "        full_df['Group'] = 'timepoint 4' \n",
    "        full_df['sample_id'] = 13\n",
    "    if (\"D\" or \"E\") and \"7\" in well:\n",
    "        full_df['treatment'] = 'paracrine' \n",
    "        full_df['Group'] = 'timepoint 5' \n",
    "        full_df['sample_id'] = 14\n",
    "    if (\"D\" or \"E\") and \"8\" in well:\n",
    "        full_df['treatment'] = 'paracrine' \n",
    "        full_df['Group'] = 'timepoint 6' \n",
    "        full_df['sample_id'] = 15\n",
    "    if (\"D\" or \"E\") and \"9\" in well:\n",
    "        full_df['treatment'] = 'paracrine' \n",
    "        full_df['Group'] = 'timepoint 7' \n",
    "        full_df['sample_id'] = 16\n",
    "\n",
    "    # Store the DataFrame in the dictionary\n",
    "    well_dataframes[well] = full_df.copy()\n",
    "    fullest_df = pd.concat([fullest_df, full_df], ignore_index=True)\n",
    "    print(len(full_df))\n",
    "    print(len(fullest_df))\n",
    "\n",
    "# Output the length of the DataFrame for each well\n",
    "for well, df in well_dataframes.items():\n",
    "    print(f'Well {well} has {len(df)} cells')\n",
    "\n",
    "fullest_df['sample_ID'] = fullest_df['sample_ID'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab86f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### It is good practice to save these dataframes at key steps, such as this one\n",
    "fullest_df.to_csv(r'your/save/path/here.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullest_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6f71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that you don't need - Preparation for conversion to AnnData object\n",
    "fullest_df = fullest_df.drop(columns=[\"Unnamed: 0\", \"bbox-0\", \"bbox-1\", \"bbox-2\",\"bbox-3\", \"orientation\", \"nuc_mask\", \"ring_mask\"])\n",
    "# Extract metadata columns and store them in a separate dataframe\n",
    "metadata = fullest_df[[\"label\", \"well\", \"treatment\", \"Group\", \"sample_id\"]]\n",
    "# Remove metadata columns from the main dataframe\n",
    "fullest_df = fullest_df.drop(columns=[\"label\", \"well\", \"treatment\", \"Group\", \"sample_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d82c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z normalize the data\n",
    "standard_df = standardizeColumns(fullest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdec0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min-max normalize the data - Not used in this example, provided for potential use\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(fullest_df)\n",
    "normalized_data = pd.DataFrame(normalized_data, index = fullest_df.index, columns = fullest_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d813fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas dataframe to an anndata object\n",
    "fullest_adata = ad.AnnData(fullest_df)\n",
    "# Add metadata back to the anndata object\n",
    "fullest_adata.obs = metadata.copy()\n",
    "fullest_adata.obs_names = [f'c_{i}' for i in fullest_adata.obs_names]\n",
    "\n",
    "#Save the entire adata file\n",
    "adata_save_path = r'your/save/path/here.h5ad'\n",
    "fullest_adata.write_h5ad(adata_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f844a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas dataframe to an anndata object\n",
    "standard_adata = ad.AnnData(standard_df)\n",
    "# Add metadata back to the anndata object\n",
    "standard_adata.obs = metadata.copy()\n",
    "standard_adata.obs_names = [f'c_{i}' for i in standard_adata.obs_names]\n",
    "\n",
    "#Save the entire adata file\n",
    "adata_save_path = r'your/save/path/here.h5ad.h5ad'\n",
    "standard_adata.write_h5ad(adata_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042b42c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Sketching lets your subsample your data accurately. \n",
    "### This example groups for subsampling based on the sample_id metadata\n",
    "\n",
    "idx, standard_adata_sub = sketch(standard_adata, num_subsamples = 1200, frequency_seed = 42, sample_set_key = 'sample_ID')\n",
    "#Save the entire adata file\n",
    "adata_save_path = r'my\\save\\path\\standard_adata_sub_sub.h5ad'\n",
    "standard_adata_sub.write_h5ad(adata_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338619b",
   "metadata": {},
   "source": [
    "Subsequent notebooks will load in the saved adata file of your choice, either the full dataset of the subsampled dataset.\n",
    "Additionally, you can produce any desired normalized or non-normalized adata file here. Z-score normalized is the provided example, but other methods can be used with minor modifications. i.e. min-max normalization using the provided code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
